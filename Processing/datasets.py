"""Utilities for building ML-ready datasets from the processed splits."""

from __future__ import annotations

from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

from .preprocessing import (
    TorqueDataPoint,
    detect_active_window_indices,
    process_data_with_cleaning,
)
from .features import extract_features_for_active_segment


def _guess_column(df: pd.DataFrame, keywords: Iterable[str]) -> Optional[str]:
    """Return the first column whose name contains any of the keywords."""

    lowered = {c.lower(): c for c in df.columns}
    for key in keywords:
        for lowered_name, original in lowered.items():
            if key in lowered_name:
                return original
    return None


def load_manifest(path: Path) -> pd.DataFrame:
    """Load the manifest CSV/JSON generated by :mod:`Processing.splits`."""

    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Manifest not found: {path}")

    if path.suffix.lower() == ".json":
        df = pd.read_json(path)
    else:
        df = pd.read_csv(path)

    required_columns = {"dst_path", "label", "injured"}
    missing = required_columns - set(df.columns)
    if missing:
        raise ValueError(
            f"Manifest at {path} is missing required columns: {sorted(missing)}"
        )

    return df


def _resolve_data_path(manifest_path: Path, entry_path: str) -> Path:
    """Resolve a path from the manifest relative to the manifest file."""

    p = Path(entry_path)
    if p.is_absolute():
        return p
    return (manifest_path.parent / p).resolve()


def load_torque_series(
    csv_path: Path,
    time_column: Optional[str] = None,
    torque_column: Optional[str] = None,
) -> List[TorqueDataPoint]:
    """Read a torque CSV into :class:`TorqueDataPoint` instances."""

    df = pd.read_csv(csv_path)
    if df.empty:
        return []

    if time_column is None:
        time_column = _guess_column(df, ["time", "t", "sec", "s"])
    if torque_column is None:
        torque_column = _guess_column(df, ["torque", "nm", "moment"])

    if time_column is None or torque_column is None:
        raise ValueError(
            f"Could not infer time/torque columns for {csv_path}. "
            "Provide --time-column/--torque-column explicitly."
        )

    series = df[[time_column, torque_column]].dropna()
    series = series.sort_values(time_column)

    return [
        TorqueDataPoint(float(row[time_column]), float(row[torque_column]))
        for _, row in series.iterrows()
    ]


def _sanitize_features(features: Dict[str, float]) -> Dict[str, float]:
    clean: Dict[str, float] = {}
    for key, value in features.items():
        if value is None:
            clean[key] = np.nan
            continue
        try:
            numeric = float(value)
        except (TypeError, ValueError):
            continue
        if not np.isfinite(numeric):
            clean[key] = np.nan
        else:
            clean[key] = numeric
    return clean


def _augment_active_features(
    active: List[TorqueDataPoint],
    features: Dict[str, float],
) -> Dict[str, float]:
    augmented = dict(features)
    if not active:
        return augmented

    torques = np.array([p.torque for p in active], dtype=float)
    times = np.array([p.time for p in active], dtype=float)

    peak_idx = int(np.argmax(torques))
    augmented.update(
        {
            "active_duration_s": float(times[-1] - times[0]) if len(times) > 1 else 0.0,
            "active_peak_torque": float(torques[peak_idx]),
            "active_time_to_peak": float(times[peak_idx] - times[0]),
            "active_mean_torque": float(torques.mean()),
            "active_std_torque": float(torques.std(ddof=1)) if len(torques) > 1 else 0.0,
        }
    )
    return augmented


def build_feature_table(
    manifest_path: Path,
    *,
    sampling_rate_hz: int = 1000,
    time_column: Optional[str] = None,
    torque_column: Optional[str] = None,
    start_threshold_pct_of_peak: float = 0.05,
    end_threshold_pct_of_peak: float = 0.25,
    plateau_window_ms: int = 500,
    plateau_step_ms: int = 25,
    plateau_cov_cap: Optional[float] = None,
) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:
    """Construct an ML dataset (X, y, meta) from the split manifest."""

    manifest_path = Path(manifest_path)
    manifest = load_manifest(manifest_path)

    feature_rows: List[Dict[str, float]] = []
    labels: List[int] = []
    meta_rows: List[Dict[str, object]] = []

    for record in manifest.to_dict("records"):
        data_path = _resolve_data_path(manifest_path, record["dst_path"])
        try:
            series = load_torque_series(data_path, time_column, torque_column)
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"[warn] Skipping {data_path}: {exc}")
            continue

        if not series:
            continue

        cleaned = process_data_with_cleaning(series, sampling_rate_hz)
        if not cleaned:
            continue

        window = detect_active_window_indices(
            cleaned,
            sampling_rate_hz=sampling_rate_hz,
            start_threshold_pct_of_peak=start_threshold_pct_of_peak,
            end_threshold_pct_of_peak=end_threshold_pct_of_peak,
        )

        active = cleaned[window["startIdx"] : window["endIdx"] + 1]
        if len(active) < 5:
            continue

        feature_dict = extract_features_for_active_segment(
            active,
            onset_time=float(window["startTime"]),
            sr=sampling_rate_hz,
            plateau_window_ms=plateau_window_ms,
            plateau_step_ms=plateau_step_ms,
            plateau_cov_cap=plateau_cov_cap,
        )

        feature_dict = _augment_active_features(active, feature_dict)
        sanitized = _sanitize_features(feature_dict)
        if not sanitized:
            continue

        feature_rows.append(sanitized)
        labels.append(int(record["injured"]))
        meta_rows.append(
            {
                "path": str(data_path),
                "label": record["label"],
                "injured": int(record["injured"]),
                "session": record.get("session"),
                "trial": record.get("trial"),
                "side": record.get("side"),
                "group": record.get("group"),
            }
        )

    if not feature_rows:
        raise RuntimeError(
            "No features could be extracted. Ensure the manifest and CSV files "
            "are present and column names are correct."
        )

    all_columns = sorted({k for row in feature_rows for k in row.keys()})
    X = pd.DataFrame([{col: row.get(col, np.nan) for col in all_columns} for row in feature_rows])
    y = pd.Series(labels, name="injured")
    meta = pd.DataFrame(meta_rows)
    return X, y, meta

